{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantize Saudi-Judge to AWQ for vLLM\n",
        "\n",
        "This notebook quantizes your 14B Qwen3 model to AWQ 4-bit format using **llm-compressor** (the official vLLM quantization tool).\n",
        "\n",
        "**Requirements:**\n",
        "- RunPod GPU Pod with A100 (40GB+)  \n",
        "- Hugging Face account with write access\n",
        "\n",
        "**Runtime:** ~30-45 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: upgrade pip if needed\n",
        "!pip install --upgrade pip -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Install llm-compressor (official vLLM quantization tool)\n",
        "# Uses system torch - no need to reinstall\n",
        "\n",
        "%pip install llmcompressor -q\n",
        "%pip install transformers accelerate huggingface_hub datasets hf_transfer -q\n",
        "\n",
        "print(\"✅ Dependencies installed!\")\n",
        "print(\"\\n⚠️  RESTART THE RUNTIME NOW before continuing!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1b: Verify installation (run AFTER restarting runtime)\n",
        "import torch\n",
        "import transformers\n",
        "import llmcompressor\n",
        "\n",
        "print(f\"torch: {torch.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"llmcompressor: {llmcompressor.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"\\n✅ Ready to quantize!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"\"  # <-- PASTE YOUR TOKEN\n",
        "login(token=HF_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Prepare calibration dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"Aljalajil/Saudi-Judge-Merged-16bit\"\n",
        "OUTPUT_DIR = \"Saudi-Judge-AWQ\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Load calibration dataset (256 samples is good for AWQ)\n",
        "NUM_CALIBRATION_SAMPLES = 256\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\n",
        "ds = ds.shuffle(seed=42)\n",
        "\n",
        "def preprocess(example):\n",
        "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n",
        "\n",
        "ds = ds.map(preprocess)\n",
        "\n",
        "print(f\"✅ Loaded {len(ds)} calibration samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Load model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID}\")\n",
        "print(\"This takes 5-10 minutes for a 14B model...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 5: Quantize to AWQ 4-bit using llm-compressor\n",
        "from llmcompressor import oneshot\n",
        "from llmcompressor.modifiers.awq import AWQModifier\n",
        "\n",
        "# AWQ recipe: 4-bit weights, 16-bit activations, asymmetric\n",
        "recipe = [\n",
        "    AWQModifier(\n",
        "        ignore=[\"lm_head\"],\n",
        "        scheme=\"W4A16_ASYM\",\n",
        "        targets=[\"Linear\"],\n",
        "        duo_scaling=\"both\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(\"Starting AWQ quantization...\")\n",
        "print(\"This takes 20-40 minutes. Please be patient!\")\n",
        "\n",
        "oneshot(\n",
        "    model=model,\n",
        "    dataset=ds,\n",
        "    recipe=recipe,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
        ")\n",
        "\n",
        "print(\"✅ Quantization complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 6: Push directly to Hugging Face Hub\n",
        "REPO_NAME = \"Aljalajil/Saudi-Judge-AWQ\"\n",
        "\n",
        "print(f\"Pushing model to {REPO_NAME}...\")\n",
        "model.push_to_hub(REPO_NAME, save_compressed=True, private=True)\n",
        "tokenizer.push_to_hub(REPO_NAME, private=True)\n",
        "\n",
        "print(f\"\"\"\n",
        "========================================\n",
        "✅ QUANTIZATION & UPLOAD COMPLETE!\n",
        "\n",
        "Your model is now available at:\n",
        "https://huggingface.co/{REPO_NAME}\n",
        "\n",
        "Next steps:\n",
        "1. Go to RunPod Serverless\n",
        "2. Edit your vLLM endpoint\n",
        "3. Set model to: {REPO_NAME}  \n",
        "4. Save and test!\n",
        "\n",
        "Don't forget to DELETE this GPU Pod!\n",
        "========================================\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# (Optional) Step 7: Save locally if needed\n",
        "# Uncomment if you want a local copy:\n",
        "# model.save_pretrained(OUTPUT_DIR, save_compressed=True)\n",
        "# tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "# print(f\"Saved to {OUTPUT_DIR}/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 8: Test the quantized model with vLLM (streaming)\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"Aljalajil/Saudi-Judge-AWQ\"\n",
        "\n",
        "# Load tokenizer for chat template\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Initialize vLLM engine\n",
        "print(f\"Loading vLLM model: {MODEL_ID}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "llm = LLM(\n",
        "    model=MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    max_model_len=8192,  # Limit context length to fit in GPU memory\n",
        "    gpu_memory_utilization=0.90,\n",
        ")\n",
        "print(\"✅ vLLM model loaded!\")\n",
        "\n",
        "# Test prompt (Arabic legal query)\n",
        "test_prompt = \"ما هي عقوبة السرقة في النظام السعودي؟\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"أنت قاضٍ سعودي متخصص في الأنظمة والقوانين السعودية.\"},\n",
        "    {\"role\": \"user\", \"content\": test_prompt}\n",
        "]\n",
        "\n",
        "# Apply chat template to get the formatted prompt\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\nTesting: {test_prompt}\\n\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Response: \", end=\"\")\n",
        "\n",
        "# Generate with vLLM\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    max_tokens=512,\n",
        "    stop=[\"<|im_end|>\", \"<|endoftext|>\", \"<|im_start|>\"],\n",
        "    stop_token_ids=[151645, 151643],\n",
        ")\n",
        "\n",
        "# Generate (vLLM doesn't support streaming in generate(), use batch generation)\n",
        "outputs = llm.generate([prompt], sampling_params)\n",
        "print(outputs[0].outputs[0].text)\n",
        "\n",
        "print(\"\\n✅ Test complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}